{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install node2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnWd93aIQcsZ",
        "outputId": "c1e1165d-ae19-4afe-a9bd-adb1275e672b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: node2vec in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.3.3)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.5.1)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (3.5)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from node2vec) (1.26.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from node2vec) (4.67.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim<5.0.0,>=4.3.0->node2vec) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.0->node2vec) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.neighbors import NearestNeighbors # Not used in the provided snippet\n",
        "# from scipy.sparse import csr_matrix # Not used in the provided snippet\n",
        "# from scipy.sparse.linalg import eigsh # Not used in the provided snippet, np.linalg.eig is used\n",
        "from node2vec import Node2Vec\n",
        "import umap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "import re # Added import re\n",
        "\n",
        "def load_classifier_components(path_prefix='/content/drive/MyDrive/ASLPaper/political_classifier'):\n",
        "   # Ensure the path is correct for your Colab environment\n",
        "   try:\n",
        "       with open(f'{path_prefix}_model.pkl', 'rb') as f:\n",
        "           model = pickle.load(f)\n",
        "       with open(f'{path_prefix}_tfidf.pkl', 'rb') as f:\n",
        "           tfidf = pickle.load(f)\n",
        "       with open(f'{path_prefix}_label_encoder.pkl', 'rb') as f:\n",
        "           label_encoder = pickle.load(f)\n",
        "       with open(f'{path_prefix}_features.pkl', 'rb') as f:\n",
        "           feature_names = pickle.load(f)\n",
        "       print(\"Classifier components loaded successfully.\")\n",
        "       return model, tfidf, label_encoder, feature_names\n",
        "   except FileNotFoundError as e:\n",
        "       print(f\"Error loading classifier component: {e}\")\n",
        "       print(f\"Please check the path: {path_prefix}\")\n",
        "       return None, None, None, None\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "   # Ensure text is a string\n",
        "   if not isinstance(text, str):\n",
        "       return \"\"\n",
        "   text = text.lower()\n",
        "   text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text) # More robust URL removal\n",
        "   text = re.sub(r'@\\w+|#\\w+', '', text) # Remove mentions and hashtags text\n",
        "   text = re.sub(r'[^\\w\\s]', '', text) # Remove punctuation\n",
        "   return text.strip()\n",
        "\n",
        "def extract_custom_features(tweets_df_input): # Expects a DataFrame with 'tweet' column\n",
        "   # Make a copy to avoid SettingWithCopyWarning if tweets_df_input is a slice\n",
        "   tweets_df = tweets_df_input.copy()\n",
        "   features = pd.DataFrame(index=tweets_df.index) # Preserve index\n",
        "   features['length'] = tweets_df['tweet'].str.len().fillna(0)\n",
        "   features['num_words'] = tweets_df['tweet'].str.split().str.len().fillna(0)\n",
        "   # Original tweet (before preprocess_tweet) should be used for #, @, http counts\n",
        "   # If 'tweet' column in tweets_df is already preprocessed, these counts might be 0\n",
        "   # Assuming 'tweet' column here is the raw tweet for these specific counts.\n",
        "   # If not, this part might need adjustment based on when preprocessing happens for these features.\n",
        "   features['num_hashtags'] = tweets_df['tweet'].str.count('#').fillna(0)\n",
        "   features['num_mentions'] = tweets_df['tweet'].str.count('@').fillna(0)\n",
        "   features['num_urls'] = tweets_df['tweet'].str.count(r'http|www').fillna(0)\n",
        "   return features\n",
        "\n",
        "# Load G early if it's used for filtering later, but it's not used before user_topic_normalized\n",
        "print(\"Loading network G...\")\n",
        "try:\n",
        "    G = nx.read_gexf(\"/content/drive/MyDrive/ASLPaper/political_network.gexf\")\n",
        "    print(f\"Network G loaded with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: political_network.gexf not found. Please check the path.\")\n",
        "    G = nx.Graph() # Create an empty graph to avoid errors later, though it will limit network-based analysis\n",
        "except Exception as e:\n",
        "    print(f\"Error loading GEXF: {e}\")\n",
        "    G = nx.Graph()\n",
        "\n",
        "\n",
        "# Load classifier components\n",
        "model, tfidf, label_encoder, custom_feature_names_loaded = load_classifier_components()\n",
        "if not all([model, tfidf, label_encoder, custom_feature_names_loaded]):\n",
        "    print(\"Exiting due to failure in loading classifier components.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# Modified classify_tweets_with_confidence to preserve index\n",
        "def classify_tweets_with_confidence(tweets_series, model, tfidf, label_encoder, custom_feature_names, confidence_threshold=0.4):\n",
        "    # tweets_series is a Pandas Series with an index\n",
        "    temp_df_for_processing = pd.DataFrame({'tweet': tweets_series}) # Index is preserved\n",
        "    temp_df_for_processing['clean_text'] = temp_df_for_processing['tweet'].apply(preprocess_tweet)\n",
        "\n",
        "    X_tfidf = tfidf.transform(temp_df_for_processing['clean_text'])\n",
        "\n",
        "    # extract_custom_features expects a DataFrame with 'tweet' column\n",
        "    # We pass the temp_df_for_processing which has the original 'tweet' text and index\n",
        "    custom_features_df = extract_custom_features(temp_df_for_processing[['tweet']])\n",
        "\n",
        "    custom_features_ordered = pd.DataFrame(columns=custom_feature_names, index=temp_df_for_processing.index)\n",
        "    for col in custom_feature_names:\n",
        "        if col in custom_features_df.columns:\n",
        "            custom_features_ordered[col] = custom_features_df[col]\n",
        "        else:\n",
        "            custom_features_ordered[col] = 0\n",
        "    custom_features_ordered = custom_features_ordered.fillna(0)\n",
        "\n",
        "    # Ensure no NaN values in features before hstack\n",
        "    X_tfidf_array = X_tfidf.toarray()\n",
        "    custom_features_array = custom_features_ordered.values\n",
        "    if np.isnan(X_tfidf_array).any() or np.isnan(custom_features_array).any():\n",
        "        print(\"Warning: NaN values found in feature arrays before hstack. Attempting to fill with 0.\")\n",
        "        X_tfidf_array = np.nan_to_num(X_tfidf_array)\n",
        "        custom_features_array = np.nan_to_num(custom_features_array)\n",
        "\n",
        "    X_combined = np.hstack([X_tfidf_array, custom_features_array])\n",
        "\n",
        "    predictions = model.predict(X_combined)\n",
        "    probabilities = model.predict_proba(X_combined)\n",
        "\n",
        "    results = pd.DataFrame(index=tweets_series.index) # Preserve original index\n",
        "    results['original_tweet'] = tweets_series.values\n",
        "    results['predicted_topic_code'] = predictions\n",
        "    results['confidence'] = probabilities.max(axis=1)\n",
        "\n",
        "    # Handle potential issues if label_encoder wasn't fit on all possible codes\n",
        "    # or if predictions array contains unexpected values.\n",
        "    # Ensure all predicted codes are within the range of known classes\n",
        "    max_code = len(label_encoder.classes_) - 1\n",
        "    results['predicted_topic_code'] = np.clip(results['predicted_topic_code'], 0, max_code)\n",
        "    results['predicted_topic'] = label_encoder.inverse_transform(results['predicted_topic_code'])\n",
        "\n",
        "    for i, topic_class in enumerate(label_encoder.classes_):\n",
        "        results[f'prob_{topic_class}'] = probabilities[:, i]\n",
        "\n",
        "    results['final_prediction'] = results.apply(\n",
        "        lambda x: x['predicted_topic'] if x['confidence'] >= confidence_threshold else 'UNCERTAIN',\n",
        "        axis=1\n",
        "    )\n",
        "    return results\n",
        "\n",
        "# --- Chunked Data Loading and Classification ---\n",
        "json_chunk_size = 20000  # Adjust based on Colab RAM; 50k might still be too large for some datasets/models\n",
        "all_classified_results = []\n",
        "processed_tweets_count = 0\n",
        "\n",
        "print(f\"Starting to process tweets_2.json in chunks of {json_chunk_size}...\")\n",
        "try:\n",
        "    # Use iterator for tqdm to get total number of chunks if possible (requires knowing file length or pre-counting lines)\n",
        "    # For simplicity, if total is not known, tqdm won't show a total progress bar but will show iterations.\n",
        "    # To get total for tqdm with read_json chunksize, one might need to count lines first.\n",
        "    # For now, let's proceed without total in tqdm for pd.read_json iterator.\n",
        "\n",
        "    # Estimate total chunks for tqdm if possible (optional, for better progress bar)\n",
        "    # This is a rough estimate and might not be perfectly accurate.\n",
        "    try:\n",
        "        with open('/content/drive/MyDrive/ASLPaper/tweets_2.json', 'r') as f_count:\n",
        "            total_lines = sum(1 for line in f_count)\n",
        "        total_json_chunks = int(np.ceil(total_lines / json_chunk_size))\n",
        "        print(f\"Estimated total JSON chunks: {total_json_chunks}\")\n",
        "    except:\n",
        "        total_json_chunks = None # Unable to estimate\n",
        "        print(\"Could not estimate total JSON chunks for progress bar.\")\n",
        "\n",
        "    json_iterator = pd.read_json('/content/drive/MyDrive/ASLPaper/tweets_2.json', lines=True, chunksize=json_chunk_size)\n",
        "\n",
        "    for json_chunk_df in tqdm(json_iterator, total=total_json_chunks, desc=\"Processing JSON file\"):\n",
        "        if 'text' not in json_chunk_df.columns or 'screen_name' not in json_chunk_df.columns:\n",
        "            print(\"Skipping chunk due to missing 'text' or 'screen_name' column.\")\n",
        "            continue\n",
        "\n",
        "        json_chunk_df.dropna(subset=['text', 'screen_name'], inplace=True)\n",
        "        json_chunk_df['text'] = json_chunk_df['text'].astype(str) # Ensure text is string\n",
        "\n",
        "        if json_chunk_df.empty:\n",
        "            continue\n",
        "\n",
        "        processed_tweets_count += len(json_chunk_df)\n",
        "\n",
        "        # Classify the current JSON chunk\n",
        "        # The 'text' series passed to classify_tweets_with_confidence will have its original index from json_chunk_df\n",
        "        chunk_results_df = classify_tweets_with_confidence(\n",
        "           json_chunk_df['text'], model, tfidf, label_encoder,\n",
        "           custom_feature_names_loaded, confidence_threshold=0.4\n",
        "        )\n",
        "\n",
        "        # Align screen_name using the preserved index\n",
        "        chunk_results_df['screen_name'] = json_chunk_df.loc[chunk_results_df.index, 'screen_name']\n",
        "        all_classified_results.append(chunk_results_df)\n",
        "\n",
        "        # Optional: delete to free memory, though Python's GC should handle it\n",
        "        # del json_chunk_df\n",
        "        # del chunk_results_df\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: tweets_2.json not found. Please check the path.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during JSON processing or classification: {e}\")\n",
        "    exit()\n",
        "\n",
        "if not all_classified_results:\n",
        "   print(\"No tweets were classified. Exiting.\")\n",
        "   exit()\n",
        "\n",
        "print(f\"Finished processing {processed_tweets_count} tweets from JSON file.\")\n",
        "print(\"Concatenating classification results...\")\n",
        "classified_tweets_df = pd.concat(all_classified_results, ignore_index=False) # Preserve original indices if they are unique and meaningful\n",
        "# If indices are not unique across chunks (e.g. each chunk starts from 0), then use ignore_index=True\n",
        "# However, for screen_name alignment, we relied on original indices from json_chunk_df.\n",
        "# If json_chunk_df indices are globally unique (e.g. if read_json assigns a unique range), then ignore_index=False is fine.\n",
        "# If json_chunk_df indices are 0-based for each chunk, then screen_name alignment was correct *within* the chunk processing.\n",
        "# For pd.concat, if original indices are not globally unique, using ignore_index=True is safer for the final big DataFrame.\n",
        "# Let's assume the index alignment for screen_name was handled correctly within the loop.\n",
        "# For concat, if the original indices from tweets_2.json were unique, we might want to keep them.\n",
        "# If not, resetting index is fine.\n",
        "# Given the .loc usage for screen_name, it implies indices from json_chunk_df['text'] were meaningful.\n",
        "# Let's try to keep them if possible, otherwise reset.\n",
        "# For safety, if we are not sure about global uniqueness of indices from chunked read_json:\n",
        "classified_tweets_df = pd.concat(all_classified_results).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(\"Filtering valid tweets...\")\n",
        "valid_tweets = classified_tweets_df[classified_tweets_df['final_prediction'] != 'UNCERTAIN'].copy() # Use .copy()\n",
        "total_tweets_after_concat = len(classified_tweets_df)\n",
        "valid_tweets_count = len(valid_tweets)\n",
        "\n",
        "print('valid_tweets_count: ', valid_tweets_count)\n",
        "\n",
        "if total_tweets_after_concat == 0:\n",
        "    print(\"No tweets in the concatenated DataFrame. Exiting.\")\n",
        "    exit()\n",
        "if valid_tweets_count == 0:\n",
        "    print(\"No valid tweets after filtering for confidence. Exiting.\")\n",
        "    exit()\n",
        "\n",
        "print(f\"Valid classified tweets: {valid_tweets_count}/{total_tweets_after_concat} ({100*valid_tweets_count/total_tweets_after_concat:.1f}%)\")\n",
        "\n",
        "print(\"Creating user-topic matrix...\")\n",
        "user_topic_matrix = valid_tweets.pivot_table(\n",
        "   index='screen_name',\n",
        "   columns='final_prediction',\n",
        "   # values='confidence', # Using 'confidence' to count, any column would do.\n",
        "   # For aggfunc='count', a column that exists for all rows is needed. 'original_tweet' or 'predicted_topic_code'\n",
        "   values='original_tweet', # Or any other column that is present\n",
        "   aggfunc='count',\n",
        "   fill_value=0\n",
        ")\n",
        "\n",
        "user_topic_normalized = user_topic_matrix.div(user_topic_matrix.sum(axis=1).replace(0, 1), axis=0) # Avoid division by zero for users with no valid tweets in matrix\n",
        "\n",
        "print(\"Filtering users and graph...\")\n",
        "users_in_graph = set(G.nodes())\n",
        "users_with_tweets = set(user_topic_normalized.index)\n",
        "valid_users = list(users_in_graph.intersection(users_with_tweets))\n",
        "\n",
        "if not valid_users:\n",
        "    print(\"No common users between the graph and tweet data. Cannot proceed with network analysis.\")\n",
        "    # Optionally, proceed with PCA on all users from user_topic_normalized if G_filtered is not essential\n",
        "    # For now, exiting if no valid_users for network-based steps.\n",
        "    exit()\n",
        "\n",
        "user_topic_filtered = user_topic_normalized.loc[valid_users]\n",
        "G_filtered = G.subgraph(valid_users).copy()\n",
        "print(f\"Filtered graph to {G_filtered.number_of_nodes()} users with tweet data.\")\n",
        "\n",
        "\n",
        "X = user_topic_filtered.values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "yUo-HAaSBML2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_tweets.columns"
      ],
      "metadata": {
        "id": "Qf6Fsnb3NyF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_tweets.predicted_topic.value_counts()"
      ],
      "metadata": {
        "id": "BqRrlDnYSJVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_diffusion_operator(graph, x_scaled_features, user_list, alpha=0.7, epsilon=None):\n",
        "   n = len(x_scaled_features)\n",
        "\n",
        "   if epsilon is None:\n",
        "       # Pairwise distances only for users in graph\n",
        "       distances_squared = np.sum((x_scaled_features[:, np.newaxis] - x_scaled_features[np.newaxis, :])**2, axis=2)\n",
        "       # Ensure distances are non-negative before sqrt, though squaring and summing should ensure this.\n",
        "       # distances = np.sqrt(distances_squared)\n",
        "       # Using median of non-zero squared distances to avoid issues with sqrt(0) if epsilon is for squared distances\n",
        "       # Or, if epsilon is for non-squared, calculate sqrt then median.\n",
        "       # The formula exp(-dist^2 / (2*epsilon)) suggests epsilon might be related to variance (sigma^2)\n",
        "       # Let's assume epsilon is for the squared distance scale, or it's a bandwidth parameter.\n",
        "       # If it's a bandwidth for exp(-dist^2 / (2*epsilon^2)), then epsilon is like sigma.\n",
        "       # If it's exp(-dist^2 / epsilon_prime), then epsilon_prime is 2*sigma^2.\n",
        "       # Original code: exp(-norm(...)^2 / (2 * epsilon)) -> implies epsilon is variance like.\n",
        "\n",
        "       # Let's use median of squared distances to avoid issues with very small epsilon\n",
        "       # if distances are small.\n",
        "       if n > 1:\n",
        "           sq_distances_flat = distances_squared[np.triu_indices(n, k=1)] # Upper triangle without diagonal\n",
        "           if len(sq_distances_flat[sq_distances_flat > 1e-9]) > 0: # Avoid issues with all-zero distances\n",
        "               epsilon = np.median(sq_distances_flat[sq_distances_flat > 1e-9])\n",
        "           else:\n",
        "               epsilon = 1.0 # Default if all distances are zero or too small\n",
        "           if epsilon < 1e-9: epsilon = 1.0 # Prevent too small epsilon\n",
        "       else:\n",
        "           epsilon = 1.0\n",
        "       print(f\"Calculated epsilon for content similarity: {epsilon:.4f}\")\n",
        "\n",
        "   # Content similarity based on features of users in the graph\n",
        "   content_similarity_sq_dist = np.sum((x_scaled_features[:, np.newaxis] - x_scaled_features[np.newaxis, :])**2, axis=2)\n",
        "   content_similarity = np.exp(-content_similarity_sq_dist / (2 * epsilon)) # Should be n_valid_users x n_valid_users\n",
        "\n",
        "   # Adjacency matrix for G_filtered, ensuring nodelist matches x_scaled_features order\n",
        "   adj_matrix = nx.adjacency_matrix(graph, nodelist=user_list).toarray() # n_valid_users x n_valid_users\n",
        "\n",
        "   W = alpha * content_similarity + (1 - alpha) * adj_matrix\n",
        "\n",
        "   D_diag = np.sum(W, axis=1)\n",
        "   # D_inv = np.diag(1.0 / (D_diag + 1e-10)) # Adding small constant for stability\n",
        "   # P = D_inv @ W\n",
        "\n",
        "   # For eigsh, we often use the normalized Laplacian L_sym = I - D^{-1/2} W D^{-1/2}\n",
        "   # Or for random walk P = D^{-1}W\n",
        "   # Let's stick to P = D^{-1}W as in the original code's intent for np.linalg.eig\n",
        "   D_inv_sqrt_diag = 1.0 / (np.sqrt(D_diag) + 1e-10)\n",
        "   D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n",
        "   # L_sym = np.eye(n) - D_inv_sqrt @ W @ D_inv_sqrt # Symmetric Normalized Laplacian\n",
        "\n",
        "   # For P = D^{-1}W\n",
        "   D_inv_diag = 1.0 / (D_diag + 1e-10)\n",
        "   P = np.diag(D_inv_diag) @ W\n",
        "\n",
        "   return P, W\n",
        "\n",
        "print(\"Constructing diffusion operator...\")\n",
        "# Pass user_topic_filtered.index which is `valid_users` list\n",
        "P, W = construct_diffusion_operator(G_filtered, X_scaled, user_topic_filtered.index.tolist())\n",
        "\n",
        "\n",
        "print(\"Performing eigendecomposition...\")\n",
        "k = 10\n",
        "try:\n",
        "    # Using eigsh for potentially large sparse P if P were sparse, but P is dense here.\n",
        "    # np.linalg.eig is fine for dense P.\n",
        "    # Ensure P is not all zeros or problematic\n",
        "    if np.allclose(P, 0):\n",
        "        raise ValueError(\"Diffusion operator P is all zeros.\")\n",
        "    eigenvalues, eigenvectors = np.linalg.eig(P)\n",
        "\n",
        "    # Sort eigenvalues and eigenvectors\n",
        "    # Note: np.linalg.eig does not guarantee order, and eigenvectors are columns\n",
        "    idx = eigenvalues.real.argsort()[::-1] # Sort by real part in descending order\n",
        "    eigenvalues = eigenvalues[idx].real # Take real part\n",
        "    eigenvectors = eigenvectors[:, idx].real # Take real part of eigenvectors\n",
        "\n",
        "    eigenvalues = eigenvalues[:k]\n",
        "    eigenvectors = eigenvectors[:, :k]\n",
        "\n",
        "except np.linalg.LinAlgError as e:\n",
        "    print(f\"Eigen decomposition failed: {e}. Using PCA as fallback for diffusion embedding.\")\n",
        "    pca_diff = PCA(n_components=min(k-1, X_scaled.shape[1], X_scaled.shape[0]-1 if X_scaled.shape[0]>1 else 1), random_state=42)\n",
        "    diffusion_embedding = pca_diff.fit_transform(X_scaled)\n",
        "    if diffusion_embedding.shape[1] < k-1: # Pad if PCA returned fewer components\n",
        "        padding = np.zeros((diffusion_embedding.shape[0], k-1 - diffusion_embedding.shape[1]))\n",
        "        diffusion_embedding = np.hstack([diffusion_embedding, padding])\n",
        "\n",
        "else:\n",
        "    # The first eigenvector (for eigenvalue 1) is often constant for P of a connected graph.\n",
        "    # The \"Fiedler vector\" is typically the second one used for spectral clustering/embedding.\n",
        "    # Diffusion maps often use eigenvectors starting from the 2nd.\n",
        "    if eigenvectors.shape[1] > 1 :\n",
        "        diffusion_embedding = eigenvectors[:, 1:k] * eigenvalues[1:k] # Weighted by eigenvalues\n",
        "    elif eigenvectors.shape[1] == 1: # Only one eigenvector, use it (less ideal)\n",
        "        print(\"Warning: Only one eigenvector obtained. Diffusion embedding quality might be affected.\")\n",
        "        diffusion_embedding = eigenvectors[:, 0:1] * eigenvalues[0:1]\n",
        "        # Pad if k > 1\n",
        "        if k-1 > diffusion_embedding.shape[1]:\n",
        "             padding = np.zeros((diffusion_embedding.shape[0], k-1 - diffusion_embedding.shape[1]))\n",
        "             diffusion_embedding = np.hstack([diffusion_embedding, padding])\n",
        "    else: # No eigenvectors\n",
        "        print(\"Warning: No eigenvectors obtained. Using zeros for diffusion embedding.\")\n",
        "        diffusion_embedding = np.zeros((X_scaled.shape[0], k-1))\n",
        "\n",
        "\n",
        "print(\"Running Node2Vec...\")\n",
        "# Ensure G_filtered is not empty and has edges for Node2Vec\n",
        "if G_filtered.number_of_nodes() > 0 and G_filtered.number_of_edges() > 0:\n",
        "    node2vec = Node2Vec(G_filtered, dimensions=32, walk_length=20, num_walks=80, workers=4, quiet=True)\n",
        "    model_n2v = node2vec.fit(window=10, min_count=1, batch_words=100)\n",
        "    node_embeddings = np.array([model_n2v.wv[str(node)] for node in user_topic_filtered.index]) # Use user_topic_filtered.index\n",
        "else:\n",
        "    print(\"Warning: G_filtered is empty or has no edges. Using zeros for node_embeddings.\")\n",
        "    node_embeddings = np.zeros((len(user_topic_filtered.index), 32))\n",
        "\n",
        "\n",
        "# Ensure diffusion_embedding has the correct number of rows\n",
        "if diffusion_embedding.shape[0] != node_embeddings.shape[0]:\n",
        "    print(f\"Warning: Mismatch in embedding shapes. Diffusion: {diffusion_embedding.shape}, Node2Vec: {node_embeddings.shape}\")\n",
        "    # Attempt to reconcile if one is a subset of the other, or fallback\n",
        "    # This case should ideally not happen if nodelists are handled consistently\n",
        "    # For now, if mismatch, fall back to using only one type or zeros for the problematic one\n",
        "    # A simple fallback: if diffusion_embedding is problematic, use only node_embeddings, or vice-versa\n",
        "    # Or, if node_embeddings is the issue (e.g. G_filtered was empty), use only diffusion_embedding\n",
        "    if node_embeddings.shape[0] == 0 and diffusion_embedding.shape[0] > 0 :\n",
        "         combined_embeddings = diffusion_embedding\n",
        "    elif diffusion_embedding.shape[0] == 0 and node_embeddings.shape[0] > 0:\n",
        "         combined_embeddings = node_embeddings\n",
        "    elif diffusion_embedding.shape[0] != node_embeddings.shape[0]: # If still mismatched and both non-empty\n",
        "        print(\"Cannot combine embeddings due to shape mismatch. Using only diffusion_embedding if available, else node_embeddings.\")\n",
        "        if diffusion_embedding.shape[0] == len(user_topic_filtered.index):\n",
        "            combined_embeddings = diffusion_embedding\n",
        "        elif node_embeddings.shape[0] == len(user_topic_filtered.index):\n",
        "            combined_embeddings = node_embeddings\n",
        "        else: # Critical error\n",
        "            print(\"Fatal error in embedding shapes. Exiting.\")\n",
        "            exit()\n",
        "    else: # Shapes match\n",
        "        combined_embeddings = np.hstack([diffusion_embedding, node_embeddings])\n",
        "\n",
        "else: # Shapes match\n",
        "    combined_embeddings = np.hstack([diffusion_embedding, node_embeddings])\n",
        "\n",
        "\n",
        "print(\"Running UMAP for trajectory mapping...\")\n",
        "# Ensure combined_embeddings is not empty\n",
        "if combined_embeddings.shape[0] == 0:\n",
        "    print(\"Error: Combined embeddings are empty. Cannot run UMAP.\")\n",
        "    exit()\n",
        "# Adjust UMAP n_neighbors if number of samples is too small\n",
        "n_neighbors_umap = min(30, combined_embeddings.shape[0] - 1) if combined_embeddings.shape[0] > 1 else 1\n",
        "if n_neighbors_umap <= 0: n_neighbors_umap = 1 # Ensure positive\n",
        "\n",
        "trajectory_mapper = umap.UMAP(\n",
        "   n_neighbors=n_neighbors_umap,\n",
        "   min_dist=0.5,\n",
        "   n_components=2, # For 2D visualization\n",
        "   metric='euclidean',\n",
        "   spread=1.5,\n",
        "   repulsion_strength=0.5,\n",
        "   negative_sample_rate=5,\n",
        "   random_state=42\n",
        ")\n",
        "trajectory_embedding = trajectory_mapper.fit_transform(combined_embeddings)\n",
        "\n",
        "print(\"Calculating pseudo-time using PCA on UMAP embedding...\")\n",
        "pca = PCA(n_components=1)\n",
        "\n",
        "\n",
        "pseudo_time_raw = pca.fit_transform(trajectory_embedding).flatten()\n",
        "# Normalize pseudo_time to 0-1 range\n",
        "if pseudo_time_raw.max() - pseudo_time_raw.min() > 1e-9: # Avoid division by zero\n",
        "    pseudo_time = (pseudo_time_raw - pseudo_time_raw.min()) / (pseudo_time_raw.max() - pseudo_time_raw.min())\n",
        "else:\n",
        "    pseudo_time = np.zeros_like(pseudo_time_raw)\n",
        "\n",
        "\n",
        "user_pseudo_time_df = pd.DataFrame({\n",
        "   'user': user_topic_filtered.index, # Use index from user_topic_filtered\n",
        "   'pseudo_time': pseudo_time,\n",
        "   'x': trajectory_embedding[:, 0],\n",
        "   'y': trajectory_embedding[:, 1]\n",
        "})\n",
        "\n",
        "# Get dominant topic for each user (from valid tweets only)\n",
        "user_dominant_topic = valid_tweets.groupby('screen_name')['final_prediction'].agg(lambda x: x.mode()[0] if not x.mode().empty else \"Unknown\")\n",
        "user_pseudo_time_df = user_pseudo_time_df.merge(\n",
        "   user_dominant_topic.to_frame('dominant_topic'),\n",
        "   left_on='user',\n",
        "   right_index=True,\n",
        "   how='left' # Keep all users from pseudo_time_df, fill missing dominant_topic if any\n",
        ")\n",
        "user_pseudo_time_df['dominant_topic'].fillna(\"Unknown\", inplace=True)\n",
        "\n",
        "\n",
        "print(\"Plotting results...\")\n",
        "plt.figure(figsize=(14, 10), dpi=150)\n",
        "scatter = plt.scatter(\n",
        "   user_pseudo_time_df['x'],\n",
        "   user_pseudo_time_df['y'],\n",
        "   c=user_pseudo_time_df['pseudo_time'],\n",
        "   cmap='viridis', # Viridis is perceptually uniform and good for sequential data\n",
        "   s=50,\n",
        "   alpha=0.7,\n",
        "   edgecolors='black',\n",
        "   linewidth=0.5\n",
        ")\n",
        "plt.colorbar(scatter, label='Pseudo-time (Ideological Position)')\n",
        "plt.xlabel('UMAP Dimension 1 (Ideological Space)', fontsize=12)\n",
        "plt.ylabel('UMAP Dimension 2 (Ideological Space)', fontsize=12)\n",
        "plt.title('Political Ideological Spectrum via Pseudo-time Analysis (UMAP)', fontsize=16)\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "\n",
        "# Trajectory Line (Smoothed)\n",
        "if len(user_pseudo_time_df) > 3 : # Need enough points for spline\n",
        "    from scipy.interpolate import UnivariateSpline # Keep import local if not used elsewhere often\n",
        "    sorted_users_for_traj = user_pseudo_time_df.sort_values('pseudo_time')\n",
        "    trajectory_line = sorted_users_for_traj[['x', 'y']].values\n",
        "\n",
        "    t = np.linspace(0, 1, len(trajectory_line))\n",
        "    # Adjust smoothing factor 's' based on number of points; None for interpolation, or a positive value for smoothing\n",
        "    s_factor = len(trajectory_line) * 0.05 # Heuristic, adjust as needed\n",
        "\n",
        "    # Check for sufficient unique x values for spline on t\n",
        "    if len(np.unique(t)) > 3: # UnivariateSpline needs at least k+1 unique points (default k=3)\n",
        "        spline_x = UnivariateSpline(t, trajectory_line[:, 0], s=s_factor, k=min(3, len(np.unique(t))-1) )\n",
        "        spline_y = UnivariateSpline(t, trajectory_line[:, 1], s=s_factor, k=min(3, len(np.unique(t))-1) )\n",
        "\n",
        "        t_smooth = np.linspace(0, 1, 300) # More points for smoother curve\n",
        "        x_smooth = spline_x(t_smooth)\n",
        "        y_smooth = spline_y(t_smooth)\n",
        "        plt.plot(x_smooth, y_smooth, color='red', alpha=0.5, linewidth=2.5, label='Smoothed Ideological Trajectory')\n",
        "    else:\n",
        "        print(\"Not enough unique points to draw a smoothed trajectory line.\")\n",
        "        plt.plot(trajectory_line[:,0], trajectory_line[:,1], color='red', alpha=0.3, linewidth=2, linestyle='--', label='Raw Trajectory Path')\n",
        "\n",
        "\n",
        "# Annotate extreme users\n",
        "num_annotate = min(5, len(user_pseudo_time_df) // 2 if len(user_pseudo_time_df) > 0 else 0)\n",
        "if num_annotate > 0:\n",
        "    extremes = pd.concat([\n",
        "        user_pseudo_time_df.nsmallest(num_annotate, 'pseudo_time'),\n",
        "        user_pseudo_time_df.nlargest(num_annotate, 'pseudo_time')\n",
        "    ]).drop_duplicates()\n",
        "\n",
        "    for _, user_data in extremes.iterrows():\n",
        "       plt.annotate(\n",
        "           str(user_data['user'])[:15], # Ensure user is string for slicing\n",
        "           (user_data['x'], user_data['y']),\n",
        "           xytext=(5, 5),\n",
        "           textcoords='offset points',\n",
        "           fontsize=7, # Smaller font for annotations\n",
        "           alpha=0.8,\n",
        "           bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", lw=0.5, alpha=0.6)\n",
        "       )\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('political_pseudotime_spectrum_umap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot by dominant topic\n",
        "plt.figure(figsize=(14, 10), dpi=150)\n",
        "unique_topics = sorted(user_pseudo_time_df['dominant_topic'].unique())\n",
        "# Ensure enough colors if many topics\n",
        "if len(unique_topics) <= 10:\n",
        "    topic_colors_list = plt.cm.get_cmap('tab10', len(unique_topics)).colors\n",
        "elif len(unique_topics) <= 20:\n",
        "    topic_colors_list = plt.cm.get_cmap('tab20', len(unique_topics)).colors\n",
        "else:\n",
        "    topic_colors_list = plt.cm.get_cmap('nipy_spectral', len(unique_topics))(np.linspace(0,1,len(unique_topics)))\n",
        "\n",
        "topic_color_map = dict(zip(unique_topics, topic_colors_list))\n",
        "\n",
        "for topic in unique_topics:\n",
        "   topic_users = user_pseudo_time_df[user_pseudo_time_df['dominant_topic'] == topic]\n",
        "   if not topic_users.empty:\n",
        "       plt.scatter(\n",
        "           topic_users['x'],\n",
        "           topic_users['y'],\n",
        "           color=topic_color_map[topic], # Direct color assignment\n",
        "           label=topic,\n",
        "           s=50,\n",
        "           alpha=0.7,\n",
        "           edgecolors='black',\n",
        "           linewidth=0.5\n",
        "       )\n",
        "\n",
        "plt.xlabel('UMAP Dimension 1 (Ideological Space)', fontsize=12)\n",
        "plt.ylabel('UMAP Dimension 2 (Ideological Space)', fontsize=12)\n",
        "plt.title('Political Ideological Spectrum by Dominant Topic (UMAP)', fontsize=16)\n",
        "plt.legend(title=\"Dominant Topic\", bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.tight_layout(rect=[0,0,0.85,1]) # Adjust for legend\n",
        "plt.savefig('political_pseudotime_by_topic_umap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "user_pseudo_time_df.to_csv('political_pseudotime_results_umap.csv', index=False)\n",
        "\n",
        "if not user_pseudo_time_df.empty and 'pseudo_time' in user_pseudo_time_df.columns:\n",
        "    print(f\"\\nPseudo-time range: [{user_pseudo_time_df['pseudo_time'].min():.3f}, {user_pseudo_time_df['pseudo_time'].max():.3f}]\")\n",
        "else:\n",
        "    print(\"\\nPseudo-time calculation did not complete successfully for all users.\")\n",
        "print(f\"Number of politicians analyzed (in final UMAP plot): {len(user_pseudo_time_df)}\")\n",
        "print(f\"Number of users in filtered graph G_filtered: {G_filtered.number_of_nodes()}\")\n",
        "print(f\"Number of users with topic proportions (valid_users for G_filtered): {len(valid_users)}\")"
      ],
      "metadata": {
        "id": "4z6kjuecLCOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trajectory Line (Smoothed and Continuous)\n",
        "if len(user_pseudo_time_df) > 10:  # Need enough points for good smoothing\n",
        "    from scipy.interpolate import UnivariateSpline, interp1d\n",
        "    from scipy.signal import savgol_filter\n",
        "    from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "    # Sort by pseudo-time\n",
        "    sorted_users = user_pseudo_time_df.sort_values('pseudo_time')\n",
        "\n",
        "    # Use isotonic regression to ensure monotonic progression along pseudo-time\n",
        "    iso_reg_x = IsotonicRegression(increasing=True)\n",
        "    iso_reg_y = IsotonicRegression(increasing=True)\n",
        "\n",
        "    # Create cumulative distance along the trajectory for better parameterization\n",
        "    points = sorted_users[['x', 'y']].values\n",
        "    diffs = np.diff(points, axis=0)\n",
        "    distances = np.sqrt(np.sum(diffs**2, axis=1))\n",
        "    cumulative_dist = np.concatenate([[0], np.cumsum(distances)])\n",
        "    cumulative_dist_normalized = cumulative_dist / cumulative_dist[-1]\n",
        "\n",
        "    # Apply Savitzky-Golay filter for initial smoothing\n",
        "    window_length = min(31, len(points) // 4 * 2 + 1)  # Ensure odd number\n",
        "    if window_length >= 5:\n",
        "        x_smooth_initial = savgol_filter(points[:, 0], window_length, 3)\n",
        "        y_smooth_initial = savgol_filter(points[:, 1], window_length, 3)\n",
        "    else:\n",
        "        x_smooth_initial = points[:, 0]\n",
        "        y_smooth_initial = points[:, 1]\n",
        "\n",
        "    # Create interpolation functions using cumulative distance\n",
        "    # Use cubic interpolation for smoothness\n",
        "    f_x = interp1d(cumulative_dist_normalized, x_smooth_initial, kind='cubic',\n",
        "                   bounds_error=False, fill_value='extrapolate')\n",
        "    f_y = interp1d(cumulative_dist_normalized, y_smooth_initial, kind='cubic',\n",
        "                   bounds_error=False, fill_value='extrapolate')\n",
        "\n",
        "    # Generate many points along the normalized distance\n",
        "    t_fine = np.linspace(0, 1, 1000)\n",
        "    x_interp = f_x(t_fine)\n",
        "    y_interp = f_y(t_fine)\n",
        "\n",
        "    # Apply additional smoothing with larger window\n",
        "    final_window = min(201, len(t_fine) // 2 * 2 + 1)\n",
        "    if final_window >= 5:\n",
        "        x_final = savgol_filter(x_interp, final_window, 3)\n",
        "        y_final = savgol_filter(y_interp, final_window, 3)\n",
        "    else:\n",
        "        x_final = x_interp\n",
        "        y_final = y_interp\n",
        "\n",
        "    # Optional: Apply spline for ultra-smooth result\n",
        "    # Use much higher smoothing factor for relaxed fit\n",
        "    s_factor = len(points) * 2.0  # Increased smoothing\n",
        "\n",
        "    try:\n",
        "        # Parametric spline fitting\n",
        "        spline_x = UnivariateSpline(t_fine, x_final, s=s_factor, k=3)\n",
        "        spline_y = UnivariateSpline(t_fine, y_final, s=s_factor, k=3)\n",
        "\n",
        "        # Generate final smooth trajectory\n",
        "        t_plot = np.linspace(0, 1, 500)\n",
        "        x_trajectory = spline_x(t_plot)\n",
        "        y_trajectory = spline_y(t_plot)\n",
        "\n",
        "    except:\n",
        "        # Fallback to interpolated values if spline fails\n",
        "        x_trajectory = x_final[::2]  # Downsample for plotting\n",
        "        y_trajectory = y_final[::2]\n",
        "\n",
        "    # Plot the smooth trajectory\n",
        "    plt.plot(x_trajectory, y_trajectory,\n",
        "             color='red',\n",
        "             alpha=0.6,\n",
        "             linewidth=3,\n",
        "             label='Ideological Trajectory',\n",
        "             zorder=1)  # Put behind scatter points\n",
        "\n",
        "    # Add gradient coloring to show progression\n",
        "    for i in range(len(x_trajectory)-1):\n",
        "        progress = i / (len(x_trajectory)-1)\n",
        "        plt.plot(x_trajectory[i:i+2], y_trajectory[i:i+2],\n",
        "                color=plt.cm.coolwarm(progress),\n",
        "                linewidth=2.5,\n",
        "                alpha=0.4)\n",
        "\n",
        "    # Optionally add direction arrows\n",
        "    arrow_indices = np.linspace(0, len(x_trajectory)-1, 10, dtype=int)[1:-1]\n",
        "    for idx in arrow_indices:\n",
        "        if idx > 0 and idx < len(x_trajectory)-1:\n",
        "            dx = x_trajectory[idx+1] - x_trajectory[idx-1]\n",
        "            dy = y_trajectory[idx+1] - y_trajectory[idx-1]\n",
        "            plt.arrow(x_trajectory[idx], y_trajectory[idx],\n",
        "                     dx*0.05, dy*0.05,\n",
        "                     head_width=0.02,\n",
        "                     head_length=0.02,\n",
        "                     fc='darkred',\n",
        "                     ec='darkred',\n",
        "                     alpha=0.3,\n",
        "                     zorder=2)\n",
        "\n",
        "else:\n",
        "    print(\"Not enough points to create smooth trajectory\")\n",
        "    # Simple line connection as fallback\n",
        "    sorted_users = user_pseudo_time_df.sort_values('pseudo_time')\n",
        "    plt.plot(sorted_users['x'], sorted_users['y'],\n",
        "            'r--', alpha=0.3, linewidth=1.5,\n",
        "            label='Raw Trajectory')"
      ],
      "metadata": {
        "id": "wi9x5qb6coJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PARAMETER GUIDE - Adjust these for different behaviors\n",
        "# ============================================\n",
        "\n",
        "# SMOOTHING PARAMETERS\n",
        "TRAJECTORY_SMOOTHING = 2.0      # Higher = smoother trajectory (0.1-5.0)\n",
        "SAVGOL_WINDOW_RATIO = 0.25     # Fraction of points for initial smoothing (0.1-0.5)\n",
        "FINAL_SMOOTH_RATIO = 0.2        # Fraction for final smoothing (0.1-0.4)\n",
        "TRAJECTORY_POINTS = 514\n",
        "\n",
        "# VISUAL PARAMETERS\n",
        "POINT_SIZE = 80                 # Size of data points (20-200)\n",
        "POINT_ALPHA = 0.8              # Transparency of points (0.1-1.0)\n",
        "EDGE_WIDTH = 0.5               # Edge width of points (0-2)\n",
        "TRAJECTORY_WIDTH = 3.5         # Width of trajectory line (1-5)\n",
        "TRAJECTORY_ALPHA = 0.4         # Transparency of trajectory (0.1-0.8)\n",
        "SHOW_ARROWS = True             # Show direction arrows on trajectory\n",
        "ARROW_COUNT = 8                # Number of arrows (5-20)\n",
        "\n",
        "# ANNOTATION PARAMETERS\n",
        "ANNOTATE_EXTREMES = False       # Annotate extreme users\n",
        "EXTREME_COUNT = 10             # Number of extremes to annotate (5-20)\n",
        "ANNOTATION_SIZE = 8            # Font size for annotations (6-12)\n",
        "\n",
        "# ============================================\n",
        "\n",
        "print(\"Plotting results with colored topics...\")\n",
        "plt.figure(figsize=(16, 12), dpi=150)\n",
        "\n",
        "# Define 10 distinct colors\n",
        "TOPIC_COLORS = {\n",
        "    0: '#FF0000',  # Red\n",
        "    1: '#0000FF',  # Blue\n",
        "    2: '#00FF00',  # Green\n",
        "    3: '#FFD700',  # Gold\n",
        "    4: '#FF00FF',  # Magenta\n",
        "    5: '#00CED1',  # Dark Turquoise\n",
        "    6: '#FF8C00',  # Dark Orange\n",
        "    7: '#4B0082',  # Indigo\n",
        "    8: '#32CD32',  # Lime Green\n",
        "    9: '#8B4513',  # Saddle Brown\n",
        "}\n",
        "\n",
        "# Plot data points colored by topic\n",
        "unique_topics = sorted(user_pseudo_time_df['dominant_topic'].unique())\n",
        "for i, topic in enumerate(unique_topics):\n",
        "    topic_users = user_pseudo_time_df[user_pseudo_time_df['dominant_topic'] == topic]\n",
        "    color = TOPIC_COLORS.get(i % 10, '#808080')  # Default to gray if >10 topics\n",
        "\n",
        "    plt.scatter(\n",
        "        topic_users['x'],\n",
        "        topic_users['y'],\n",
        "        c=color,\n",
        "        label=f'{topic}',\n",
        "        s=POINT_SIZE,\n",
        "        alpha=POINT_ALPHA,\n",
        "        edgecolors='black',\n",
        "        linewidth=EDGE_WIDTH,\n",
        "        zorder=3  # Put points on top\n",
        "    )\n",
        "\n",
        "# Smooth Trajectory\n",
        "if len(user_pseudo_time_df) > 10:\n",
        "    from scipy.interpolate import UnivariateSpline, interp1d\n",
        "    from scipy.signal import savgol_filter\n",
        "\n",
        "    sorted_users = user_pseudo_time_df.sort_values('pseudo_time')\n",
        "    points = sorted_users[['x', 'y']].values\n",
        "\n",
        "    # Calculate cumulative distance\n",
        "    diffs = np.diff(points, axis=0)\n",
        "    distances = np.sqrt(np.sum(diffs**2, axis=1))\n",
        "    cumulative_dist = np.concatenate([[0], np.cumsum(distances)])\n",
        "    cumulative_dist_normalized = cumulative_dist / cumulative_dist[-1]\n",
        "\n",
        "    # Initial smoothing with Savitzky-Golay\n",
        "    window_length = int(len(points) * SAVGOL_WINDOW_RATIO)\n",
        "    window_length = max(5, window_length if window_length % 2 == 1 else window_length + 1)\n",
        "\n",
        "    if window_length <= len(points):\n",
        "        x_smooth = savgol_filter(points[:, 0], window_length, min(3, window_length-2))\n",
        "        y_smooth = savgol_filter(points[:, 1], window_length, min(3, window_length-2))\n",
        "    else:\n",
        "        x_smooth = points[:, 0]\n",
        "        y_smooth = points[:, 1]\n",
        "\n",
        "    # Cubic interpolation\n",
        "    f_x = interp1d(cumulative_dist_normalized, x_smooth, kind='cubic',\n",
        "                   bounds_error=False, fill_value='extrapolate')\n",
        "    f_y = interp1d(cumulative_dist_normalized, y_smooth, kind='cubic',\n",
        "                   bounds_error=False, fill_value='extrapolate')\n",
        "\n",
        "    # Generate smooth trajectory\n",
        "    t_fine = np.linspace(0, 1, 1000)\n",
        "    x_interp = f_x(t_fine)\n",
        "    y_interp = f_y(t_fine)\n",
        "\n",
        "    # Final smoothing\n",
        "    final_window = int(len(t_fine) * FINAL_SMOOTH_RATIO)\n",
        "    final_window = max(5, final_window if final_window % 2 == 1 else final_window + 1)\n",
        "\n",
        "    if final_window <= len(t_fine):\n",
        "        x_final = savgol_filter(x_interp, final_window, min(3, final_window-2))\n",
        "        y_final = savgol_filter(y_interp, final_window, min(3, final_window-2))\n",
        "    else:\n",
        "        x_final = x_interp\n",
        "        y_final = y_interp\n",
        "\n",
        "    # Apply spline with adjustable smoothing\n",
        "    s_factor = len(points) * TRAJECTORY_SMOOTHING\n",
        "\n",
        "    try:\n",
        "        spline_x = UnivariateSpline(t_fine, x_final, s=s_factor, k=3)\n",
        "        spline_y = UnivariateSpline(t_fine, y_final, s=s_factor, k=3)\n",
        "\n",
        "        t_plot = np.linspace(0, 1, TRAJECTORY_POINTS)\n",
        "        x_trajectory = spline_x(t_plot)\n",
        "        y_trajectory = spline_y(t_plot)\n",
        "    except:\n",
        "        x_trajectory = x_final[::2]\n",
        "        y_trajectory = y_final[::2]\n",
        "\n",
        "    # Plot main trajectory\n",
        "    plt.plot(x_trajectory, y_trajectory,\n",
        "             color='black',\n",
        "             alpha=TRAJECTORY_ALPHA,\n",
        "             linewidth=TRAJECTORY_WIDTH,\n",
        "             label='Ideological Trajectory',\n",
        "             zorder=2)\n",
        "\n",
        "    # Add gradient coloring\n",
        "    for i in range(len(x_trajectory)-1):\n",
        "        progress = i / (len(x_trajectory)-1)\n",
        "        plt.plot(x_trajectory[i:i+2], y_trajectory[i:i+2],\n",
        "                color=plt.cm.RdBu(progress),\n",
        "                linewidth=TRAJECTORY_WIDTH-0.5,\n",
        "                alpha=TRAJECTORY_ALPHA*0.7,\n",
        "                zorder=1)\n",
        "\n",
        "    # Add direction arrows\n",
        "    if SHOW_ARROWS:\n",
        "        arrow_indices = np.linspace(0, len(x_trajectory)-1, ARROW_COUNT, dtype=int)[1:-1]\n",
        "        for idx in arrow_indices:\n",
        "            if 0 < idx < len(x_trajectory)-1:\n",
        "                dx = x_trajectory[idx+1] - x_trajectory[idx-1]\n",
        "                dy = y_trajectory[idx+1] - y_trajectory[idx-1]\n",
        "                norm = np.sqrt(dx**2 + dy**2)\n",
        "                if norm > 0:\n",
        "                    dx, dy = dx/norm, dy/norm\n",
        "                    plt.arrow(x_trajectory[idx], y_trajectory[idx],\n",
        "                             dx*0.03, dy*0.03,\n",
        "                             head_width=0.015,\n",
        "                             head_length=0.015,\n",
        "                             fc='black',\n",
        "                             ec='black',\n",
        "                             alpha=0.5,\n",
        "                             zorder=4)\n",
        "\n",
        "# Annotate extreme users\n",
        "if ANNOTATE_EXTREMES:\n",
        "    extremes = pd.concat([\n",
        "        user_pseudo_time_df.nsmallest(EXTREME_COUNT//2, 'pseudo_time'),\n",
        "        user_pseudo_time_df.nlargest(EXTREME_COUNT//2, 'pseudo_time')\n",
        "    ]).drop_duplicates()\n",
        "\n",
        "    for _, user_data in extremes.iterrows():\n",
        "        plt.annotate(\n",
        "            str(user_data['user'])[:20],\n",
        "            (user_data['x'], user_data['y']),\n",
        "            xytext=(5, 5),\n",
        "            textcoords='offset points',\n",
        "            fontsize=ANNOTATION_SIZE,\n",
        "            alpha=0.9,\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", lw=0.5, alpha=0.7),\n",
        "            zorder=5\n",
        "        )\n",
        "\n",
        "# Add pseudo-time colorbar\n",
        "scatter_collection = plt.scatter(user_pseudo_time_df['x'], user_pseudo_time_df['y'],\n",
        "                                c=user_pseudo_time_df['pseudo_time'],\n",
        "                                cmap='coolwarm', s=0, alpha=0)\n",
        "cbar = plt.colorbar(scatter_collection, label='Pseudo-time Position', pad=0.02)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "\n",
        "plt.xlabel('UMAP Dimension 1 (Ideological Space)', fontsize=14)\n",
        "plt.ylabel('UMAP Dimension 2 (Ideological Space)', fontsize=14)\n",
        "plt.title('Political Ideological Spectrum with Topic Distribution', fontsize=18)\n",
        "plt.legend(title=\"Topics\", bbox_to_anchor=(1.15, 1), loc='upper left',\n",
        "          ncol=1 if len(unique_topics) > 10 else 1, fontsize=10)\n",
        "plt.grid(True, linestyle='--', alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('political_pseudotime_colored_topics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\nTopic Distribution:\")\n",
        "topic_counts = user_pseudo_time_df['dominant_topic'].value_counts()\n",
        "for topic, count in topic_counts.items():\n",
        "    print(f\"  {topic}: {count} users ({count/len(user_pseudo_time_df)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nTotal users plotted: {len(user_pseudo_time_df)}\")\n",
        "print(f\"Pseudo-time range: [{user_pseudo_time_df['pseudo_time'].min():.3f}, {user_pseudo_time_df['pseudo_time'].max():.3f}]\")"
      ],
      "metadata": {
        "id": "--GW4fsMXuLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRAJECTORY ALIGNMENT PARAMETERS\n",
        "# ============================================\n",
        "ENFORCE_TRAJECTORY = True       # Whether to apply trajectory enforcement\n",
        "TRAJECTORY_STRENGTH = 0.3      # How strongly to pull points to trajectory (0.0-1.0)\n",
        "BANDWIDTH_FACTOR = 2.0         # Bandwidth for gaussian weights (0.5-5.0)\n",
        "LOCAL_DENSITY_PRESERVE = 0.7   # Preserve local density (0.0-1.0)\n",
        "REPULSION_STRENGTH = 0.5       # Repulsion between points (0.0-2.0)\n",
        "\n",
        "print(\"Running UMAP for trajectory mapping with trajectory enforcement...\")\n",
        "\n",
        "# Add missing imports\n",
        "from scipy.spatial.distance import pdist\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "\n",
        "# First pass: Standard UMAP to get initial embedding\n",
        "n_neighbors_umap = min(30, combined_embeddings.shape[0] - 1) if combined_embeddings.shape[0] > 1 else 1\n",
        "\n",
        "# Use different UMAP parameters for better trajectory formation\n",
        "trajectory_mapper = umap.UMAP(\n",
        "    n_neighbors=n_neighbors_umap,\n",
        "    min_dist=0.3,              # Increased for more spread\n",
        "    n_components=2,\n",
        "    metric='euclidean',\n",
        "    spread=2.0,                # Increased spread\n",
        "    repulsion_strength=REPULSION_STRENGTH,\n",
        "    negative_sample_rate=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "initial_embedding = trajectory_mapper.fit_transform(combined_embeddings)\n",
        "\n",
        "# Calculate initial pseudo-time for trajectory\n",
        "pca_initial = PCA(n_components=1)\n",
        "pseudo_time_initial = pca_initial.fit_transform(initial_embedding).flatten()\n",
        "pseudo_time_initial = (pseudo_time_initial - pseudo_time_initial.min()) / (pseudo_time_initial.max() - pseudo_time_initial.min())\n",
        "\n",
        "if ENFORCE_TRAJECTORY and len(initial_embedding) > 20:\n",
        "    print(\"Applying trajectory enforcement...\")\n",
        "\n",
        "    # Sort by pseudo-time\n",
        "    sort_idx = np.argsort(pseudo_time_initial)\n",
        "    sorted_embedding = initial_embedding[sort_idx]\n",
        "\n",
        "    # Fit a smooth principal curve through the data\n",
        "    # Use polynomial regression for smooth curve\n",
        "    poly_degree = min(5, len(sorted_embedding) // 20)  # Adaptive degree\n",
        "\n",
        "    # Parametric curve fitting\n",
        "    t_param = np.linspace(0, 1, len(sorted_embedding))\n",
        "\n",
        "    # Fit x and y separately as functions of t\n",
        "    poly = PolynomialFeatures(degree=poly_degree)\n",
        "    t_poly = poly.fit_transform(t_param.reshape(-1, 1))\n",
        "\n",
        "    # Ridge regression for smoothness\n",
        "    ridge_x = Ridge(alpha=10.0)\n",
        "    ridge_y = Ridge(alpha=10.0)\n",
        "\n",
        "    ridge_x.fit(t_poly, sorted_embedding[:, 0])\n",
        "    ridge_y.fit(t_poly, sorted_embedding[:, 1])\n",
        "\n",
        "    # Generate smooth trajectory\n",
        "    t_smooth = np.linspace(0, 1, 1000)\n",
        "    t_smooth_poly = poly.transform(t_smooth.reshape(-1, 1))\n",
        "\n",
        "    trajectory_x = ridge_x.predict(t_smooth_poly)\n",
        "    trajectory_y = ridge_y.predict(t_smooth_poly)\n",
        "\n",
        "    # For each point, find nearest point on trajectory\n",
        "    trajectory_points = np.column_stack([trajectory_x, trajectory_y])\n",
        "\n",
        "    # Create KDTree for efficient nearest neighbor search\n",
        "    tree = KDTree(trajectory_points)\n",
        "\n",
        "    # Find nearest trajectory point for each data point\n",
        "    distances, indices = tree.query(initial_embedding, k=1)\n",
        "    nearest_trajectory_points = trajectory_points[indices.flatten()]\n",
        "\n",
        "    # Calculate gaussian weights based on distance to trajectory\n",
        "    bandwidth = np.median(distances) * BANDWIDTH_FACTOR\n",
        "    weights = np.exp(-distances.flatten()**2 / (2 * bandwidth**2))\n",
        "\n",
        "    # Apply trajectory enforcement with local density preservation\n",
        "    trajectory_embedding = (\n",
        "        (1 - TRAJECTORY_STRENGTH) * initial_embedding +\n",
        "        TRAJECTORY_STRENGTH * nearest_trajectory_points * weights.reshape(-1, 1)\n",
        "    )\n",
        "\n",
        "    # Optional: Apply local density preservation\n",
        "    if LOCAL_DENSITY_PRESERVE > 0:\n",
        "        # Use UMAP's graph to preserve local neighborhoods\n",
        "        fuzzy_simplicial_set = trajectory_mapper.graph_\n",
        "\n",
        "        # Apply spring-like forces to maintain local structure\n",
        "        n_epochs = 50\n",
        "        learning_rate = 1.0\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Random sampling of edges\n",
        "            edges = fuzzy_simplicial_set.nonzero()\n",
        "            n_edges = len(edges[0])\n",
        "\n",
        "            if n_edges > 0:\n",
        "                sample_size = min(n_edges, 5000)\n",
        "                edge_sample = np.random.choice(n_edges, sample_size, replace=False)\n",
        "\n",
        "                for idx in edge_sample:\n",
        "                    i, j = edges[0][idx], edges[1][idx]\n",
        "                    weight = fuzzy_simplicial_set[i, j]\n",
        "\n",
        "                    # Spring force between connected points\n",
        "                    diff = trajectory_embedding[j] - trajectory_embedding[i]\n",
        "                    dist = np.linalg.norm(diff)\n",
        "\n",
        "                    if dist > 0:\n",
        "                        # Attractive force\n",
        "                        force = weight * diff / dist * learning_rate * LOCAL_DENSITY_PRESERVE\n",
        "                        trajectory_embedding[i] += force * 0.5\n",
        "                        trajectory_embedding[j] -= force * 0.5\n",
        "\n",
        "            learning_rate *= 0.95  # Decay learning rate\n",
        "\n",
        "    # Final embedding\n",
        "    trajectory_embedding = trajectory_embedding\n",
        "\n",
        "else:\n",
        "    print(\"Using standard UMAP embedding without trajectory enforcement\")\n",
        "    trajectory_embedding = initial_embedding\n",
        "\n",
        "# Recalculate pseudo-time on final embedding\n",
        "pca = PCA(n_components=1)\n",
        "pseudo_time_raw = pca.fit_transform(trajectory_embedding).flatten()\n",
        "\n",
        "if pseudo_time_raw.max() - pseudo_time_raw.min() > 1e-9:\n",
        "    pseudo_time = (pseudo_time_raw - pseudo_time_raw.min()) / (pseudo_time_raw.max() - pseudo_time_raw.min())\n",
        "else:\n",
        "    pseudo_time = np.zeros_like(pseudo_time_raw)\n",
        "\n",
        "# Alternative: Use diffusion-based embedding for smoother trajectory\n",
        "if ENFORCE_TRAJECTORY and len(trajectory_embedding) > 50:\n",
        "    print(\"Applying diffusion-based smoothing...\")\n",
        "\n",
        "    # Build affinity matrix\n",
        "    gamma = 1.0 / (2.0 * np.median(pdist(trajectory_embedding))**2)\n",
        "    affinity = rbf_kernel(trajectory_embedding, gamma=gamma)\n",
        "\n",
        "    # Normalize to create diffusion operator\n",
        "    row_sum = affinity.sum(axis=1)\n",
        "    diffusion = affinity / row_sum[:, np.newaxis]\n",
        "\n",
        "    # Apply diffusion steps\n",
        "    n_diffusion_steps = 3\n",
        "    diffused_embedding = trajectory_embedding.copy()\n",
        "\n",
        "    for _ in range(n_diffusion_steps):\n",
        "        diffused_embedding = 0.5 * diffused_embedding + 0.5 * (diffusion @ diffused_embedding)\n",
        "\n",
        "    # Blend original and diffused\n",
        "    trajectory_embedding = 0.7 * trajectory_embedding + 0.3 * diffused_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHFKyHo2cQXs",
        "outputId": "1b837dee-f083-497d-fb34-65163b861823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running UMAP for trajectory mapping with trajectory enforcement...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying trajectory enforcement...\n",
            "Applying diffusion-based smoothing...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yi_r0pwhgL5Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
